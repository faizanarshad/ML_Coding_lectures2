{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Square Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The least square method is the process of finding the best-fitting curve or line of best fit for a set of data points by reducing the sum of the squares of the offsets (residual part) of the points from the curve.\n",
    ">\n",
    "> During the process of finding the relation between two variables, the trend of outcomes are estimated quantitatively. This process is termed as regression analysis. \n",
    "> \n",
    "> The method of curve fitting is an approach to regression analysis. This method of fitting equations which approximates the curves to given raw data is the least squares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The least-squares method is often applied in data fitting. The best fit result is assumed to reduce the sum of squared errors or residuals which are stated to be the differences between the observed or experimental value and corresponding fitted value given in the model.\n",
    "\n",
    "There are two basic categories of least-squares problems:\n",
    "\n",
    "    -   Ordinary or linear least squares\n",
    "    -   Nonlinear least squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  > In linear regression, the line of best fit is a straight line\n",
    "  >\n",
    "  > The given data points are to be minimized by the method of reducing residuals or offsets of each point from the line.\n",
    "  > \n",
    "  > The vertical offsets are generally used in surface, polynomial and hyperplane problems, while perpendicular offsets are utilized in common practice.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
