{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function used in Nurel Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoid Function\n",
    "    - It is a form of differentiate real function that contains the positive derivates with specific aspects of smoothness. It shows its existence in output layer of deep learning models and utilize mainly for estimating the probability-based outputs. \n",
    "\n",
    " \n",
    "## Hyperbolic Tangent Function\n",
    "\n",
    "    - This is another form of function which is a zero-centric function that lies within -1 to 1. It is much more extensively used for multilayer neural networks. But, it cannot solve the eliminating gradient problem. It is a tanh function. \n",
    "\n",
    "## Softmax Function\n",
    "\n",
    "\t- It returns a vector of K values into vector of K real values that summarize into 1. The inputs can be positive, negative or xero but the return value will be between 0 and 1. \n",
    "\n",
    "## Softsign Function\n",
    "\t\n",
    "    -It is the form of quadratic polynomial that converge exponentially. It rescales the value between the -1 and 1 by applying threshold. \n",
    "\n",
    "## Rectified Linear Unit Function \n",
    "\n",
    "    - It is like tanh and sigmoid function, but it provides opportunity to create better performance in optimizing gradient-descent methods. It provides faster computation. It rectifies the values less than zero into zero. \n",
    "\n",
    "## Exponential Linear Units Function\n",
    "\n",
    "    -   It works by identifying the positive values and improving properties of learning model. For negative value it moves them near to zero.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
